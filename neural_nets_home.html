<!DOCTYPE html>
<html lang="eng">
    <head>
        <meta charset="UTF-8">
        <meta name="veiwport" content="width=device-qwidth, initial-scale=1.0">
        <title>AI Learning Roadmap</title>
        <style>
            ul { margin-left: 25px; }
            ol { margin-left: 10px; }
        </style>
        <link rel="stylesheet" href="styles/python.css">
    </head>

    <body>
    <div class="menu">
        <a href="index.html">Home</a>
        <a href="learning_hub.html">Leaning Hub</a>
        <a href="neural_nets_home.html">Neural Networks Home</a>
        <a href="https://humansideoftek.blogspot.com/">Blog</a>
    </div>

    <div class="container">
        <div class="topics">

            <button><a href="learning_hub.html">Learning Hub Home</button></a>
            <button><a href="neural_nets_home.html">Neural Networks</a></button>
            <button><a href="python.html">Python</a></button>
            <button><a href="javascript.html">Javascript</a></button>
            <button><a href="agentic_AI.html">Agentic System</a></button>
        </div>

        <div class="content">
            <div class="titleContainer">
                <div class="mainTitle">
                    <h1 id="titleEx">Neural Network Learning Hub!</h1>
                </div>
            </div>

            <div id="textContainer" class="textContainer">

            <h1>AI Learning Roadmap</h1><br/>

            <!-- SECTION 1 -->
            <h2>Section 1: Foundations (Math, Regression, Classification, XOR)</h2><br/>
            <p>These four projects build the mental model of linear algebra → optimization → nonlinearity.</p><br/>

            <ol>
                <li><a href="https://humansideoftek.blogspot.com/2026/01/the-big-picture-linear-regression-is.html", 
                target="_blank">Single‑Feature Linear Regression</a><br/>

                <ul><br/>
                    <li>MSE, gradients, update</li>
                    <li> r, R², OLS</li>
                    <li>Geometry of projection</li>
                </ul>
                </li><br/>

                <li>
                <a href="https://humansideoftek.blogspot.com/2026/01/project-2-multifeature-linear.html",
                target="_blank">Multi‑Feature Regression</a><br/>
                
                <ul><br/>
                    <li>Vectorized dot products</li>
                    <li>Weight vector as a direction in feature space</li>
                    <li>Plane instead of line</li>
                    <li>Residual vectors</li>
                </ul>
                </li><br/>

                <li>
                <a href="https://humansideoftek.blogspot.com/2026/02/project-3-binary-classification.html",
                target="blank">Binary Classification</a><br/>
                <ul><br/>
                    <li>Logistic regression</li>
                    <li>Sigmoid</li>
                    <li>Cross‑entropy</li>
                    <li>Decision boundary geometry</li>
                </ul>
                </li><br/>

                <li>
                <a href="https://humansideoftek.blogspot.com/2026/01/solving-xor-problem-why-linear-models.html",
                target="blank">Neural Network for XOR</a><br/>
                <ul><br/>
                    <li>Hidden layer</li>
                    <li>Activation functions</li>
                    <li>Why linear models fail</li>
                    <li>Geometry of separating non‑linearly separable data</li>
                </ul>
                </li><br/>

                <li>
                <a href="https://humansideoftek.blogspot.com/2026/02/project-5-design-matrix.html",
                target="blank">The Design Matrix</a><br/>
                <ul><br/>
                    <li>Why ML always uses a matrix</li>
                    <li>How neural networks generalize it</li>
                    <li>How shapes flow through the system</li>
                    <li>How this is a neural network layer</li>
                </ul>
                </li><br/>
            </ol><br />

            <a href="intro_nn.html", target="_blank">Neural Networks Simple Math</a>
            3 images explaining what these 5 projects teach.</p><br/>
            

            <!-- SECTION 2 -->
            <h2>Section 2: Building Real Neural Networks (Projects 6–10)</h2><br/>
            <p>This is where you move from single neurons to full, modular neural network architectures—first from scratch, then with PyTorch.</p><br/>

            <ol start="6">
                <li>
                <a href="https://humansideoftek.blogspot.com/2026/02/neural-network-framework.html"
                target="_blank">Build a Neural Network Class From Scratch</a><br/>
                <ul><br/>
                    <li>A Layer class</li>
                    <li>A NeuralNetwork class</li>
                    <li>Forward pass</li>
                    <li>Backprop</li>
                    <li>Training loop</li>
                    <li>Loss functions</li>
                    <li><a href="https://humansideoftek.blogspot.com/2026/02/neural-network-activation-functions.html",
                        target="_blank">Activation functions</a></li>
                    <li>Modular architecture</li>
                </ul>
                </li><br/>

                <li>
                <a href="https://humansideoftek.blogspot.com/2026/02/project-7-introduction-to-pytorch.html",
                target="_blank">Introduction to PyTorch (Tensors, Autograd, Modules)</a><br/>
                <ul><br/>
                    <li>How PyTorch replaces your manual gradients</li>
                    <li>How nn.Module mirrors your custom class</li>
                    <li>How autograd works</li>
                    <li>How optimizers work</li>
                    <li>How to rewrite your Project 5 network in PyTorch</li>
                </ul>
                </li><br/>

                <li>
                <a href="https://humansideoftek.blogspot.com/2026/02/project-8-recurrent-neural-networks.html",
                target="_blank">Recurrent Neural Networks (RNNs)</a><br/>
                <ul><br/>
                    <li>Sequence modeling</li>
                    <li>Hidden state</li>
                    <li>Unrolling</li>
                    <li>Vanishing gradients</li>
                    <li>PyTorch’s nn.RNN</li>
                </ul>
                </li><br/>

                <li>
                <a href="#">LSTM Networks</a><br/>
                <ul><br/>
                    <li>The natural evolution of Project 7</li>
                    <li>Gates</li>
                    <li>Cell state</li>
                    <li>Long‑term memory</li>
                    <li>Why LSTMs solve vanishing gradients</li>
                    <li>PyTorch’s nn.LSTM</li>
                </ul>
                </li><br/>

                <li>
                <a href="#">GRU or CNN (Your Choice)</a><br/>
                <ul><br/>
                    <li>Explore GRU as a simplified gated RNN, or</li>
                    <li>Explore CNNs for spatial feature extraction</li>
                    <li>Compare trade‑offs and ideal use cases</li>
                </ul>
                </li><br/>
            </ol>

            <a href="https://humansideoftek.blogspot.com/2026/02/core-concepts-of-machine-learning.html",
            target="blank">The Core Concepts of Machine Learning</a>
            </div>
        </div>
    <script src="scripts/learning_hub.js"></script>
</body>
</html>
